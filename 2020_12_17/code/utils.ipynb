{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2510,
     "status": "ok",
     "timestamp": 1607812087966,
     "user": {
      "displayName": "Tim Johnsen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLtW2AX3LAyNX5SQ30F4NSvAD-t5z9ODfUkdZF0g=s64",
      "userId": "14698778181759089752"
     },
     "user_tz": 480
    },
    "id": "metY3Ym8EQ2q",
    "outputId": "840584ae-73d2-424f-980f-71b16ee797a5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from math import sqrt\n",
    "from math import log\n",
    "from math import isnan\n",
    "import csv\n",
    "from math import isinf\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "#from sklearn.tree.export import export_text\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from statistics import stdev as std\n",
    "from IPython.display import display, clear_output\n",
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "X6pzQf86EQ2t"
   },
   "outputs": [],
   "source": [
    "# simple timer to time events\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "        \n",
    "    def get(self):\n",
    "        value = time.time() - self.start\n",
    "        self.start = time.time()\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g5XrRvv0EQ2u"
   },
   "outputs": [],
   "source": [
    "# WARNING: DATA MUST BE GROUPED BY COUNTRY AND SORTED BY ASCENDING DATE, FORMAT CHANGES BY DATE FROM SOURCE\n",
    "# read and mungle data with number of cases from covid-19 file from european cdc:\n",
    "# https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide\n",
    "# keeps the columns ['Date', 'Cases', 'Country']\n",
    "# changes name of some countries to match population data from the UN:\n",
    "# https://population.un.org/wpp/Download/Standard/CS\n",
    "# drops some regions that do not exist in the UN population file\n",
    "# accepts data from a country if all data have a number of total days greater than days_min\n",
    "# accepts data from a country if all data have a maximum number of cases greater than cases_min\n",
    "# returns data sorted by ascending date while keeping grouped by country\n",
    "# rows which did not have an entry for that date have the number of cases set to -1\n",
    "def readCOVID19(path, days_min=2, cases_min=1):\n",
    "    \n",
    "    covid19_cases = pd.read_csv(path, encoding = \"ISO-8859-1\")\n",
    "\n",
    "    # remove all attributes except country and number of cases by date\n",
    "    covid19_cases = covid19_cases[['dateRep', 'cases', 'countriesAndTerritories']]\n",
    "    covid19_cases.columns = ['Date', 'Cases', 'Country']\n",
    "    covid19_cases['Date'] = pd.to_datetime(covid19_cases.Date)\n",
    "\n",
    "    # organize by day number\n",
    "    data = covid19_cases.sort_values(by=['Country','Date'])\n",
    "\n",
    "    # fix something so the cases an population .csv file names match up by country name\n",
    "    data['Country'] = data['Country'].str.replace('_', ' ')\n",
    "    data['Country'] = data['Country'].str.replace('CANADA', 'Canada') # Canada was entered in all-caps?\n",
    "    data = data.loc[data['Country'] != 'Cote dIvoire']\n",
    "    data = data.loc[data['Country'] != 'Kosovo']\n",
    "    data = data.loc[data['Country'] != 'Cases on an international conveyance Japan']\n",
    "    \n",
    "    # mungle loop that chooses to accept data based on previous parameters\n",
    "    # also redoes the 'Date' column to be 'Day' column which is number of days since first outbreak in that country\n",
    "    reranked = [] # array of new rows to make new dataframe at end\n",
    "    tosort = {} # dictionary that will sort country data in descending order of max number of cases\n",
    "    lastCountry = '' # keep track of country in previous row \n",
    "    lastDate = '' # keep track of date in previous row\n",
    "    dayNum = 1 # start with day 1\n",
    "    firstCase = False # log if this is the first case for given country\n",
    "    temp = [] # temp array to hold data points from a country, will either be accepted or rejected\n",
    "    skip = False # control if temp array is accepted or rejected\n",
    "    maxCases = 0 # keep track of maximum number of cases for country, to sort countries by severity\n",
    "    maxes = [] # keep track of all max cases for all countries\n",
    "    for index, row in data.iterrows():\n",
    "        \n",
    "        # get column values for this row\n",
    "        thisDate = row[0]\n",
    "        cases = row[1]\n",
    "        country = row[2]\n",
    "\n",
    "        # check if this is a new country from previous row\n",
    "        if country not in lastCountry:\n",
    "            # choose to either accept or reject data from last country\n",
    "            if len(temp) >= days_min and maxCases > cases_min:\n",
    "                # remove last entry (uncertainty if day was complete)\n",
    "                temp.pop(-1)\n",
    "                tosort[lastCountry] = [temp, maxCases] # save temp array to the tosort dictionary if accepted\n",
    "                maxes.append(maxCases)  # save list of max cases to array if accepted\n",
    "            # make a new temp array for this new country's data, and reinitialize other vars\n",
    "            temp = []\n",
    "            skip = False\n",
    "            firstCase = False\n",
    "            dayNum = 1\n",
    "            maxCases = 0\n",
    "\n",
    "        # check if this date is first case for this country\n",
    "        if cases > 0 and not firstCase:\n",
    "            lastDate = thisDate\n",
    "            firstCase = True\n",
    "\n",
    "        # if we are on or passed our first date, then log data\n",
    "        if firstCase:\n",
    "            # get change in days betwen this date and last date\n",
    "            deltaDays = (thisDate - lastDate).days\n",
    "            if deltaDays < 0:\n",
    "                print('Error data is not properly sorted by date', country, thisDate)\n",
    "            # check if the days inbetween this row and last row exceeds our threshold to accept\n",
    "            else:\n",
    "                # keep track of max vases\n",
    "                maxCases = max(cases, maxCases)\n",
    "                # add for each day in between\n",
    "                for i in range(deltaDays-1):\n",
    "                    # add data to temp array\n",
    "                    temp.append([dayNum + i + 1, -1, country, lastDate + timedelta(days=1+i), False])\n",
    "                # add for this day\n",
    "                dayNum += deltaDays\n",
    "                temp.append([dayNum, cases, country, thisDate, True])\n",
    "                \n",
    "        # log previous date and country\n",
    "        lastDate = thisDate\n",
    "        lastCountry = country\n",
    "        \n",
    "    # add values from temp array for last country if we are to accept\n",
    "    if len(temp) >= days_min and maxCases > cases_min:\n",
    "        # remove last entry (uncertainty if day was complete)\n",
    "        temp.pop(-1)\n",
    "        tosort[lastCountry] = [temp, maxCases]\n",
    "        maxes.append(maxCases)\n",
    "\n",
    "    # sort countries now by highest number of cases (mostly for visual purposes of plots)\n",
    "    while len(maxes) > 0:\n",
    "        # get next max value\n",
    "        thisMax = max(maxes)\n",
    "        # look for countries with max value\n",
    "        pops = [] # keep track of what country to remove from tosort\n",
    "        for country in tosort:\n",
    "            # get values for thsi country\n",
    "            rows = tosort[country][0]\n",
    "            maxCases = tosort[country][1]\n",
    "            # if this has max value then add to final rows\n",
    "            if thisMax == maxCases:\n",
    "                reranked = reranked + rows\n",
    "                pops.append(country) \n",
    "        # remove countries that were aded from temporary tosort dict\n",
    "        for country in pops:        \n",
    "            tosort.pop(country)\n",
    "        # remove this max from list of max cases\n",
    "        maxes = [x for x in maxes if x < thisMax]\n",
    "\n",
    "    # make new data frame with ranked, mungled data\n",
    "    data = pd.DataFrame(reranked, columns=['Day', 'Cases', 'Country', 'Date', 'Valid'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1U8L13RDEQ2w"
   },
   "outputs": [],
   "source": [
    "# plots all COVID19 data 5 at a time (5 countries per plot)\n",
    "# @data is pandas data frame with 3 columns ['Day', 'Cases', 'Country'] \n",
    "    # grouped by country, reformated by day# since first outbreak, sorted by ascending day\n",
    "# @xlabel is x-axis label to output on plots\n",
    "# @ylabel is y-axis label to output on plots\n",
    "def plot5(data, xlabel='Day Since First Case in Country', ylabel='Daily Number of Novel COVID-19 Cases'):\n",
    "    # redo plot every 5 countries\n",
    "    lastCut = 0 # get rows to cut for a plot of 5 countries\n",
    "    lastCountry = data.at[0,'Country'] # keep track of what the last country in the last row was\n",
    "    while lastCut < len(data) - 1:\n",
    "        \n",
    "        # init cut in dataframe to last cut from from plot\n",
    "        thisCut = lastCut\n",
    "        \n",
    "        # iterate through rows and add to this cut until we have reached a 6th\n",
    "        nCountries = 0 # keep track of how many countries are in current cut\n",
    "        for i in range(lastCut, len(data), 1):\n",
    "            # get country this row is frome\n",
    "            thisCountry = str(data.at[i,'Country'])\n",
    "            # check if we have added all rows from a new country\n",
    "            if thisCountry not in lastCountry or i == len(data) - 1:\n",
    "                nCountries += 1\n",
    "            # log what country the previous row is from\n",
    "            lastCountry = thisCountry\n",
    "            # check if we have 5 countries worth of data in cut\n",
    "            if nCountries > 4:\n",
    "                break\n",
    "            # otherwise increase cut by another row\n",
    "            thisCut += 1\n",
    "        \n",
    "        # plot this cut\n",
    "        fig, ax = plt.subplots(figsize=(15,7))\n",
    "        data_temp = data.iloc[lastCut:thisCut]\n",
    "        data_temp.loc[data_temp['Cases'] > -1].groupby(['Day', 'Country']).sum()['Cases'].unstack().plot(ax=ax)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        \n",
    "        # log previous cut\n",
    "        lastCut = thisCut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZI_9rPnsEQ2x"
   },
   "outputs": [],
   "source": [
    "# returns Root Mean Squared Error (RMSE) between two lists\n",
    "def root_mean_squared_error(test, predictions):\n",
    "    if len(test) == len(predictions):\n",
    "        error = 0.\n",
    "        for i in range(len(test)):\n",
    "            error += (test[i] - predictions[i]) ** 2\n",
    "        error /= float(len(test))\n",
    "        return sqrt(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Rg9uiV5TEQ2z"
   },
   "outputs": [],
   "source": [
    "# returns Root Mean Squared Percent Error (RMSPE) between two lists\n",
    "def root_mean_squared_percent_error(test, predictions):\n",
    "    if len(test) == len(predictions):\n",
    "        error = 0.\n",
    "        for i in range(len(test)):\n",
    "            error += ((test[i] - predictions[i]) / test[i]) ** 2\n",
    "        error /= float(len(test))\n",
    "        return sqrt(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ftT57XdEEQ2z"
   },
   "outputs": [],
   "source": [
    "# split into dictionary and remove 0s\n",
    "# data is pandas data frame following structure returned from readCOVID19()\n",
    "# returns a dictionary that maps country to a list of cases ordered by day number\n",
    "def data_prep_ARIMA(data):\n",
    "    # loop through data and create lists by country name\n",
    "    data_dict = {} # init dictionary\n",
    "    for i in range(len(data)):\n",
    "        # get country from row \n",
    "        country = data.at[i,'Country']\n",
    "        # check if country in dictionary\n",
    "        if country not in data_dict:\n",
    "            # if not, then intit a new list\n",
    "            data_dict[country] = []\n",
    "        # check if cases number from row is positive non-zero\n",
    "        if data.at[i,'Cases'] > 0:\n",
    "            # add to list for this country if positive non-zero\n",
    "            data_dict[country].append(data.at[i,'Cases'])\n",
    "        # otherwise add nan for ARIMA to handle missing case\n",
    "        else:\n",
    "            data_dict[country].append(np.nan)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "51K-7-O3EQ20"
   },
   "outputs": [],
   "source": [
    "# plot autocorrelation for number of lags in time-series\n",
    "# one plot for each country\n",
    "# @data_dict is a dictionary mapping countries to list of cases\n",
    "def autocorrelation_plots(data_dict):\n",
    "    for country in data_dict:\n",
    "        autocorrelation_plot([x for x in data_dict[country] if not isnan(x)])\n",
    "        plt.title(country)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "axI5Qg1eEQ20"
   },
   "outputs": [],
   "source": [
    "# optimizes hyper parameters for machine learning\n",
    "# @rerun True to rerun results, False to read saved results\n",
    "# @role 'predict' or 'estimate'\n",
    "# @samples is dict of samples to test hypers with\n",
    "def optimize_hypers(role, samples, tag, mlas = ['ENe', 'SVR', 'MLR', 'RFR']):\n",
    "    for sample in samples:\n",
    "        print ('on sample', sample)\n",
    "        # get mla results\n",
    "        #if role == 'predict':\n",
    "        #    results = mla_class_grid(sample)\n",
    "        #else:\n",
    "        results = mla_regress_grid(samples[sample], mlas)\n",
    "\n",
    "        # write results\n",
    "        with open('opt_hypers_' + role + '_' + tag, 'wb') as outfile:\n",
    "            pickle.dump(results, outfile)\n",
    "        with open('opt_hypers_' + role + '_' + tag + '.csv', 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            for mla in results:\n",
    "                for r in range(len(results[mla]['preds'])):\n",
    "                    writer.writerow([mla, np.average(np.array(results[mla]['preds'][r]))] + results[mla]['params'][r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hLr22V7sEQ20"
   },
   "outputs": [],
   "source": [
    "# optimizes preprocessing for machine learning\n",
    "# @rerun True to rerun results, False to read saved results\n",
    "# @role 'predict' or 'estimate'\n",
    "def optimize_preprocess(data, rerun, role, name, winRan, lagRan, derRan, stdRan, end_date):\n",
    "    header = ['Win', 'Lag', 'Der', 'Sta', ('Acc' if role == 'predict' else 'R2'), 'Tim', 'nTrain'] # header to display table\n",
    "\n",
    "    if rerun:\n",
    "        #iterate through different window_sizes and lags and check standardizing\n",
    "        table = [] # table for output, previewing live results\n",
    "        timer = Timer() # init timer\n",
    "        for window_size in winRan:\n",
    "            for lags in lagRan:\n",
    "                for deriv in derRan:\n",
    "                    for std in stdRan:\n",
    "                        # get data\n",
    "                        samples = {}\n",
    "                        samples['temp'] = prepare_MLA(data, window_size=window_size, lags=lags, sample_name='temp'\n",
    "                                                      ,outputs=1, label_type=('class' if role == \n",
    "                                                      'predict' else ('change' if deriv else 'next')), deriv=deriv\n",
    "                                                      , end_date=end_date)\n",
    "                        if std:\n",
    "                            if role == 'predict':\n",
    "                                samples = standardize(samples, std_labels=False)\n",
    "                            else:\n",
    "                                samples = standardize(samples, std_labels=True)\n",
    "\n",
    "                        # check if valid\n",
    "                        nTrain = len(samples['temp']['Xs'])\n",
    "                        print(nTrain)\n",
    "                        if nTrain >= 5:\n",
    "\n",
    "                            # get mla results\n",
    "                            if role == 'predict':\n",
    "                                results = mla_class_results(['LRe'], samples, [], disp=False)\n",
    "                                table.append([window_size, lags, deriv, std\n",
    "                                              , sum(results['temp']['LRe'])/len(results['temp']['LRe']), timer.get(), nTrain])\n",
    "                            else:\n",
    "                                results = mla_regress_results(['ENe'], samples, [], disp=False)\n",
    "                                table.append([window_size, lags, deriv, std\n",
    "                                              , sum(results['temp']['ENe'])/len(results['temp']['ENe']), timer.get(), nTrain])\n",
    "\n",
    "                            # show results\n",
    "                            clear_output() # clear output on screen for new table\n",
    "                            print(tabulate(table, headers=header)) # print new table\n",
    "                            samples.clear()\n",
    "        # write tabled results\n",
    "        with open('opt_preprocess_' + role + '_ENe_O_' + name, 'wb') as outfile:\n",
    "            pickle.dump(table, outfile)\n",
    "        with open('opt_preprocess_' + role + '_ENe_O_' + name + '.csv', 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(header)\n",
    "            for row in table:\n",
    "                writer.writerow(row)\n",
    "        print()\n",
    "    else:\n",
    "        # read tabled results\n",
    "        with open('opt_preprocess_' + role + '_O_' + name, 'rb' ) as infile:\n",
    "            table = pickle.load(infile)\n",
    "            print('*****************************')\n",
    "            print('OPTIMIZED PREPROCESSING FOR DAY')\n",
    "            print('*****************************')\n",
    "            print(tabulate(table, headers=header)) # print new table\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZOZ4KEyyEQ21"
   },
   "outputs": [],
   "source": [
    "# runs an ARIMA test on data over range of p, d, q\n",
    "# @data_dict is a dictionary mapping countries to list of cases\n",
    "# @nDays_range is range of days to check in viewing window\n",
    "# @derivative_range is range of derivatives to check (0 or 1 make most sense)\n",
    "# @lag_vals a list of two numbers to add to nDays during iteration:\n",
    "    # first index is starting value for range\n",
    "    # second index is ending value for range (non-inclusive)\n",
    "# @output_ARIMA is path to file to output results as pickle \n",
    "# @header is a list of strings for header to table that is output\n",
    "def iter_ARIMA(data_dict, nDays_range, derivative_range, lag_range, output_ARIMA):\n",
    "    # initialize some vars\n",
    "    header = ['p', 'd', 'q', 'RMSE', 'RMSPE', 'Acc', 'R2', 'Con', 'Tim']\n",
    "    warnings.filterwarnings('ignore') # ignore warnings that show when datapoints do not converge\n",
    "    rmspe = {} # log rmspe for each iteration, by country\n",
    "    rmse = {} # log rmse for each iteration, by country\n",
    "    accuracy = {} # log if accurately went up or down\n",
    "    r2 = {} # log if accurately went up or down\n",
    "    params = {} # log parameters, by country\n",
    "    allTests = {} # log test case values, by country\n",
    "    allPredictions = {} # log predicted case values, by country\n",
    "    # init lists that keeps track of all results \n",
    "    rmspe['all'] = [] \n",
    "    rmse['all'] = []\n",
    "    accuracy['all'] = []\n",
    "    r2['all'] = []\n",
    "    params['all'] = []\n",
    "    allTests['all'] = []\n",
    "    allPredictions['all'] = []\n",
    "    table = [] # table for output, previewing live results\n",
    "    times = [] # list of how long each iteration takes\n",
    "    timer = Timer() # init timer\n",
    "    \n",
    "    # iterate through number of lags to look at (number of windows to predict next value)\n",
    "    for lag in lag_range:\n",
    "        # iterate through degree of derivative to take\n",
    "        for derivative in derivative_range:\n",
    "            # iterate through desired number of days in viewing window\n",
    "            for nDays in nDays_range:\n",
    "                theseTests = [] # temp list logging tests\n",
    "                thesePredictions = [] # temp list logging predictions\n",
    "                nConverged = 0 # log number of datapoints which converged\n",
    "                nTried = 0 # log number of datapoints attempted to converge\n",
    "                nCorrect = 0 # log number of correctly predicted points to go up or down\n",
    "                \n",
    "                # run ARIMA on each country\n",
    "                for country in data_dict:\n",
    "                    # get entire time-series of datapoints\n",
    "                    X = data_dict[country]\n",
    "                    # check if time-series is long enough\n",
    "                    if len(X) <= lag:\n",
    "                        continue\n",
    "                    # get first index for chunking into train and test values\n",
    "                    size = 1 + lag + derivative\n",
    "                    # chunk into train and test\n",
    "                    train, test = X[0:size], X[size:len(X)]\n",
    "                    test_conv = [] # keep track of test values which converge\n",
    "                    history = [x for x in train] # accumalating history of all values in train chunk\n",
    "                    predictions = [] # keep track of predicted values for those which converge\n",
    "                    nSuccess = len(test) # list the number of success (subtract when error happens from non-convergence)\n",
    "                    this_nCorrect = 0\n",
    "                    # test on all datapoints in test chunk\n",
    "                    for t in range(len(test)):\n",
    "                        nTried += 1 # inc data point tried\n",
    "                        if isnan(test[t]):\n",
    "                            nTried -= 1\n",
    "                            nSuccess -= 1\n",
    "                        else:\n",
    "                            # try to fit ARIMA, error will throw if not converged\n",
    "                            try:\n",
    "                                # make ARIMA model\n",
    "                                model = ARIMA(history, order=(lag, derivative, nDays))\n",
    "                                # fit ARIMA model\n",
    "                                model_fit = model.fit(maxiter=1000)\n",
    "                                # predict next value in test chunk\n",
    "                                output = model_fit.forecast()\n",
    "                                # if we are here than converged, add to success lists\n",
    "                                test_conv.append(test[t])\n",
    "                                predictions.append(output[0])\n",
    "                                # check if we accurately went up or down \n",
    "                                if test[t] > history[-1] and output[0] > history[-1]:\n",
    "                                    this_nCorrect += 1\n",
    "                                elif test[t] <= history[-1] and output[0] <= history[-1]:\n",
    "                                    this_nCorrect += 1\n",
    "                                # if an error is thrown dec the number of successes\n",
    "                            except:\n",
    "                                nSuccess -= 1\n",
    "                        # add to future history\n",
    "                        history.append(test[t])\n",
    "                    # if atleast one data point converged then log results for this country\n",
    "                    if nSuccess > 0:\n",
    "                        nConverged += nSuccess # inc number converged\n",
    "                        nCorrect += this_nCorrect # inc number correct\n",
    "                        theseTests = theseTests + test_conv # add to test list\n",
    "                        thesePredictions = thesePredictions + predictions # add to prediction list\n",
    "                        error1 = root_mean_squared_error(test_conv, predictions) # calc rmse\n",
    "                        error2 = root_mean_squared_percent_error(test_conv, predictions) # calc rmspe\n",
    "                        error3 = this_nCorrect / nSuccess # calc accuracy\n",
    "                        error4 = r2_score(test_conv, predictions) # calc rmspe\n",
    "                        # check if we need to init a list for this country in results dict\n",
    "                        if country not in rmse:\n",
    "                            rmse[country] = []\n",
    "                            rmspe[country] = []\n",
    "                            accuracy[country] = []\n",
    "                            r2[country] = []\n",
    "                            params[country] = []\n",
    "                            allTests[country] = []\n",
    "                            allPredictions[country] = []\n",
    "                        # add results to respective dict for this country\n",
    "                        rmse[country].append(error1)\n",
    "                        rmspe[country].append(error2)\n",
    "                        accuracy[country].append(error3)\n",
    "                        r2[country].append(error4)\n",
    "                        params[country].append([lag,derivative,nDays])\n",
    "                        allTests[country].append(test_conv)\n",
    "                        allPredictions[country].append(predictions)\n",
    "\n",
    "                # log results for all countries\n",
    "                this_time = timer.get() # check timer\n",
    "                times.append(this_time) # add time for iteration\n",
    "                # check if any datapoints converged for this iteration then add to results\n",
    "                if nConverged > 0:\n",
    "                    error1 = root_mean_squared_error(theseTests, thesePredictions) # calc rmse for all countries\n",
    "                    error2 = root_mean_squared_percent_error(theseTests, thesePredictions) # same for rmspe\n",
    "                    error3 = nCorrect / nConverged\n",
    "                    error4 = r2_score(theseTests, thesePredictions)\n",
    "                    params['all'].append([lag, derivative, nDays]) # add parameters checked this iter\n",
    "                    allTests['all'].append(theseTests) # add all test datapoints to list \n",
    "                    allPredictions['all'].append(thesePredictions) # add all predictions to list\n",
    "                    rmse['all'].append(error1) # add rmse\n",
    "                    rmspe['all'].append(error2) # add rmspe\n",
    "                    accuracy['all'].append(error3) # add rmspe\n",
    "                    r2['all'].append(error4) # add r2\n",
    "                    table.append([ lag, derivative, nDays, round(error1, 2), round(error2, 2), round(error3, 2)\n",
    "                                  , round(error4, 2), round(nConverged / nTried, 2), this_time ])\n",
    "                    clear_output() # clear output on screen for new table\n",
    "                    print(tabulate(table, headers=header)) # print new table\n",
    "    \n",
    "    # write pickle file for easy reading later\n",
    "    ARIMA_results = [rmspe, rmse, accuracy, params, allTests, allPredictions, table]\n",
    "    with open(output_ARIMA, 'wb') as outfile:\n",
    "        pickle.dump(ARIMA_results, outfile)\n",
    "    with open(output_ARIMA + '.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "        \n",
    "    return ARIMA_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HGXP8xkEEQ21"
   },
   "outputs": [],
   "source": [
    "# reads and displays arima output from previous runs\n",
    "# @input_ARIMA list of paths to files to read pervious results from\n",
    "# @header list of strings that are header to table\n",
    "def read_ARIMA(input_ARIMA):\n",
    "    header = ['Lags', 'Derv', 'Days', 'RMSE %', 'RMSE', 'Accuracy %', '% Converged', 'Exec Time (s)']\n",
    "    ARIMA_results = {}\n",
    "    for arima in input_ARIMA:\n",
    "        with open(arima, 'rb' ) as infile:\n",
    "            ARIMA_results[arima] = pickle.load(infile)\n",
    "            print(tabulate(ARIMA_results[arima][-1], headers=header))\n",
    "    return ARIMA_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jqSYdAxrEQ22"
   },
   "outputs": [],
   "source": [
    "# takes the moving sum of data\n",
    "# @data is pandas data frame split by ('Day', 'Cases', 'Country') \n",
    "    # assumed to be grouped by country name and sorted by day number in ascending order\n",
    "# @days_moving is window size of moving sum\n",
    "# returns dataFrame similar to @data except days are moving sums of those days \n",
    "    # there will be days_moving-1 less rows for each country\n",
    "def moving_sum(data, days_moving, constant=1):\n",
    "    # take moving sum\n",
    "    newRows = [] # keep track of new rows to make new data frame at end\n",
    "    lastCountry = '' # keep track of country of last row, tells when to reset moving window\n",
    "    counter = 0 # count number of rows viewed from country\n",
    "    idx = 0 # track index of moving window\n",
    "    window = [None] * days_moving # init moving window\n",
    "    for i in range(len(data)):\n",
    "        # get country of this row\n",
    "        country = str(data.at[i, 'Country'])\n",
    "        # check if this is the next country\n",
    "        if country not in lastCountry:\n",
    "            # if so then reset counter\n",
    "            counter = 0\n",
    "        # set value at current window location\n",
    "        window[idx] = max(0, data.at[i, 'Cases'])\n",
    "        # increment revovling window\n",
    "        idx += 1\n",
    "        if idx >= days_moving:\n",
    "            idx = 0\n",
    "        # inc number of datapoints in this country\n",
    "        counter += 1\n",
    "        # if enough datapoints, then add to newRows array to later build dataframe\n",
    "        if counter >= days_moving:\n",
    "            newRows.append([data.at[i, 'Day'], sum(window)/constant, country, data.at[i, 'Date'], data.at[i, 'Cases'] >= 0])\n",
    "        # log country of last row\n",
    "        lastCountry = country\n",
    "        \n",
    "    # create new dataframe from moving window rows\n",
    "    data2 = pd.DataFrame(newRows, columns=['Day', 'Cases', 'Country', 'Date', 'Valid'])\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UZBL5nexEQ22"
   },
   "outputs": [],
   "source": [
    "# takes the window avearge of data\n",
    "# @data is pandas data frame split by ('Day', 'Cases', 'Country') \n",
    "    # assumed to be grouped by country name and sorted by day number in ascending order\n",
    "# @days_moving is window size\n",
    "# returns dataFrame similar to @data except days are window aves of those days \n",
    "    # there will be N/(window_size) less rows for each country\n",
    "def window_ave(data, window_size, constant=1):\n",
    "    # take moving sum\n",
    "    newRows = [] # keep track of new rows to make new data frame at end\n",
    "    lastCountry = '' # keep track of country of last row, tells when to reset moving window\n",
    "    for i in range(0, len(data), window_size):\n",
    "        arr = [0] * window_size\n",
    "        complete = False\n",
    "        for j in range(window_size):\n",
    "            if i+j >= len(data):\n",
    "                break\n",
    "            # get country of this row\n",
    "            country = str(data.at[i+j, 'Country'])\n",
    "            # check if this is the next country\n",
    "            if country != lastCountry:\n",
    "                # if so then reset counter\n",
    "                i = max(0, i+j-window_size)\n",
    "                lastCountry = country\n",
    "                break\n",
    "            # set value at current window location\n",
    "            arr[j] = max(0, data.at[i+j, 'Cases'])\n",
    "            if j == window_size-1:\n",
    "                complete = True\n",
    "        # if enough datapoints, then add to newRows array to later build dataframe\n",
    "        if complete:\n",
    "            newRows.append([data.at[i, 'Day'], sum(arr)/window_size, country, data.at[i, 'Date'], data.at[i, 'Cases'] >= 0])\n",
    "        \n",
    "    # create new dataframe from moving window rows\n",
    "    data2 = pd.DataFrame(newRows, columns=['Day', 'Cases', 'Country', 'Date', 'Valid'])\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "p1IznWCmEQ22"
   },
   "outputs": [],
   "source": [
    "# creates X and Y datapoints\n",
    "# @data is a dataframe following the architecture from read_COVID19()\n",
    "# @window_size is size of moving sum\n",
    "# @lags is number of previous datapoints to put into X\n",
    "# @outputs is number of following datapoints after X to put into Y\n",
    "# @offset is the number of y-values to skip (predicting furthe rinto future)\n",
    "# @sample_name name of sample used for output\n",
    "# @label_type determines how to set Y, it is either:\n",
    "    # 'next' for count after X (DEFAULT)\n",
    "    # 'change' for the difference of count after X\n",
    "    # 'class' for either binary 0 for a decrease after X or 1 for increase\n",
    "# @test_split is optional value between [0,1] to decide the percent of test split\n",
    "# @deriv will either take or not take 1st derivative\n",
    "# @start_date will only grab labels at and after passed in date (inclusive)\n",
    "# @end_date will only grab labels before passed in date (non-inclusive)\n",
    "# @grab_countries is a list of countries to grab, default is blank to grab all\n",
    "# return data dictionary, and data prepped for arima\n",
    "def prepare_MLA(data, window_size, lags, sample_name, outputs=1, offset=0, label_type='next'\n",
    "                ,deriv=False, start_date=pd.to_datetime('1/1/2019'), end_date=pd.to_datetime('1/1/2019')\n",
    "                , grab_countries=[]):\n",
    "    \n",
    "    # set params list\n",
    "    params = {'window_size' : window_size, 'lags' : lags, 'outputs' : outputs, 'label_type' : label_type}\n",
    "        \n",
    "    # calculate moving ave of data\n",
    "    if window_size > 1:\n",
    "        data = window_ave(data, window_size)\n",
    "    \n",
    "    # create empty matricies to be filled with data chunks\n",
    "    Xs = [] # feature vectors\n",
    "    Ys = [] # labels\n",
    "    countries = [] # country names\n",
    "    days = [] # day numbers of label\n",
    "    dates = [] # dates of label\n",
    "    \n",
    "    for_ARIMA = {} # country, time series\n",
    "    last_country = ''\n",
    "    \n",
    "    # move through all rows in data\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        # get country for this row\n",
    "        country = str(data.at[i, 'Country'])\n",
    "        if grab_countries and country not in grab_countries:\n",
    "            continue\n",
    "\n",
    "        # check for out of bounds \n",
    "        if i + lags + offset + outputs > len(data):\n",
    "            break\n",
    "        elif deriv and i == 0:\n",
    "            continue\n",
    "        elif deriv and data.at[i-1, 'Country'] != country:\n",
    "            continue\n",
    "        \n",
    "        # check if enough consequetive values are from same country\n",
    "        if data.at[i + lags + outputs - 1, 'Country'] != country:\n",
    "            continue\n",
    "        \n",
    "        # check if all data points are valid\n",
    "        skip = False\n",
    "        \n",
    "        # add lags to feature vector to make a new chunk\n",
    "        x = []\n",
    "        day = [] # get day(s) of data and label\n",
    "        this_dates = [] # get dates of data and label\n",
    "        for j in range(i, i + lags, 1):\n",
    "            if data.at[j, 'Cases'] <= 1:\n",
    "                skip = True\n",
    "                break\n",
    "            if not data.at[j, 'Valid']:\n",
    "                skip = True\n",
    "                break\n",
    "            if deriv:\n",
    "                x.append(data.at[j, 'Cases'] - data.at[j-1, 'Cases'])\n",
    "            else:\n",
    "                x.append(data.at[j, 'Cases'])\n",
    "            day.append(data.at[j, 'Day'])\n",
    "            this_dates.append(data.at[j, 'Date'])\n",
    "        if skip:\n",
    "            continue\n",
    "\n",
    "        # get labels for number of outputs\n",
    "        y = []\n",
    "        for j in range(i + lags + offset, i + lags + offset + outputs, 1):\n",
    "            if data.at[j, 'Cases'] <= 1:\n",
    "                skip = True\n",
    "                break\n",
    "            if data.at[j, 'Date'] < start_date:\n",
    "                skip = True\n",
    "                break\n",
    "            if data.at[j, 'Date'] >= end_date:\n",
    "                skip = True\n",
    "                break\n",
    "            if not data.at[j, 'Valid']:\n",
    "                skip = True\n",
    "                break\n",
    "            if label_type in 'next':\n",
    "                y.append(data.at[j, 'Cases'])\n",
    "            elif label_type in 'change':\n",
    "                y.append(int(data.at[j, 'Cases']) - int(data.at[j - 1, 'Cases']))\n",
    "            elif label_type in 'class':\n",
    "                y.append(1 if int(data.at[j, 'Cases']) > int(data.at[j - 1, 'Cases']) else 0)\n",
    "            day.append(int(data.at[j, 'Day']))\n",
    "            this_dates.append(data.at[j, 'Date'])\n",
    "        if skip:\n",
    "            continue\n",
    "        \n",
    "        # add chunk to matrices\n",
    "        Xs.append(x)\n",
    "        Ys.append(y)\n",
    "        countries.append(country)\n",
    "        days.append(day)\n",
    "        dates.append(this_dates)\n",
    "        \n",
    "        # add to arima prep\n",
    "        if last_country != country:\n",
    "            if sample_name == 'train':\n",
    "                for_ARIMA[country] = [] + x\n",
    "            else:\n",
    "                for_ARIMA[country] = []\n",
    "        for_ARIMA[country] = for_ARIMA[country] + y\n",
    "        last_country = country    \n",
    "        \n",
    "    #write pickle file\n",
    "    pickle_out = {'data' : data, 'Xs' : Xs, 'Ys' : Ys, 'countries' : countries\n",
    "                  , 'days' : days, 'dates' : dates, 'params' : params}\n",
    "#     with open('sample_' + sample_name, 'wb') as outfile:\n",
    "#         pickle.dump(pickle_out, outfile)\n",
    "        \n",
    "    return pickle_out, for_ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pC1cjq32EQ23"
   },
   "outputs": [],
   "source": [
    "# standardizes data to have a mean of 0 and standard deviation of 1 for all lags\n",
    "# @samples is a dict of map of chunks as returned from prepare_MLA()\n",
    "# @std_labels is a bool that controls to standardize labels or not\n",
    "def standardize(samples, std_labels=False, write=False):\n",
    "   \n",
    "    new_samples = {}\n",
    "    for sample_name in samples:\n",
    "        sample = samples[sample_name]\n",
    "\n",
    "        # get old data matrices\n",
    "        X = sample['Xs']\n",
    "        Y = sample['Ys']\n",
    "        countries = sample['countries']\n",
    "        days = sample['days']\n",
    "\n",
    "        # create new empty matricies to be filled with standardized data chunks\n",
    "        means = [None] * len(X)\n",
    "        stds = [None] * len(X)\n",
    "        new_Xs = [None] * len(X)\n",
    "        new_Ys = [None] * len(X)\n",
    "\n",
    "        # enumerate through and standardize\n",
    "        idx = 0\n",
    "        for i, x in enumerate(X):  \n",
    "            y = Y[i]\n",
    "            ave = sum(x) / len(x)\n",
    "            std = ( sum([((x_j - ave) ** 2) for x_j in x]) / len(x) )**0.5\n",
    "            if std == 0:\n",
    "                continue\n",
    "\n",
    "            new_x = [None] * len(x)\n",
    "            for j, x_j in enumerate(x):\n",
    "                new_x[j] = (x_j - ave) / std\n",
    "\n",
    "            if std_labels:\n",
    "                new_y = [None] * len(y)\n",
    "                for j, y_j in enumerate(y):\n",
    "                    new_y[j] = (y_j - ave) / std\n",
    "            else:\n",
    "                new_y = [y_j for y_j in y]\n",
    "\n",
    "            means[idx] = ave\n",
    "            stds[idx] = std\n",
    "            new_Xs[idx] = new_x\n",
    "            new_Ys[idx] = new_y\n",
    "            idx += 1\n",
    "        new_Xs = new_Xs[:idx]\n",
    "        new_Ys = new_Ys[:idx]\n",
    "        means = means[:idx]\n",
    "        stds = stds[:idx]\n",
    "\n",
    "        new_samples[sample_name] = {'data' : sample['data'], 'Xs' : new_Xs, 'Ys' : new_Ys, 'countries' : sample['countries']\n",
    "                                    , 'days' :  sample['days'], 'params' : sample['params'], 'sample_name' : sample_name\n",
    "                                   , 'means' : means, 'stds' : stds, 'dates' : sample['dates']}\n",
    "        \n",
    "        #write pickle file\n",
    "        if write:\n",
    "            with open('sample_std_' + sample_name, 'wb') as outfile:\n",
    "                pickle.dump(new_samples[sample_name], outfile)\n",
    "\n",
    "    return new_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jZ9Ew9rnEQ23"
   },
   "outputs": [],
   "source": [
    "# standardizes data to have a mean of 0 and standard deviation of 1 for each day#\n",
    "# @samples is a dict of map of chunks as returned from prepare_MLA()\n",
    "# @means and @stds are the means and standard deviations mapped by day# for each sample\n",
    "    # if these are passed in as parameters then the function will transform the data\n",
    "    # if these are empty then the function will calculate them, transform data, and return means and stds\n",
    "# @std_labels is a bool that controls to standardize labels or not\n",
    "def standardize_by_day(samples, means = {}, stds = {}, std_labels=False):\n",
    "   \n",
    "    new_samples = {}\n",
    "    for sample_name in samples:\n",
    "        sample = samples[sample_name]\n",
    "        \n",
    "        # create new empty matricies to be filled with standardized data chunks\n",
    "        new_Xs = [] # feature vectors\n",
    "        new_Ys = [] # labels\n",
    "\n",
    "        # get old data matrices\n",
    "        X = sample['Xs']\n",
    "        Y = sample['Ys']\n",
    "        countries = sample['countries']\n",
    "        days = sample['days']\n",
    "        params = sample['params']\n",
    "\n",
    "        # map if we have used this country/day already in sum\n",
    "        used = {}\n",
    "\n",
    "        # find means and std and transform data\n",
    "        if sample_name not in means:\n",
    "            means[sample_name] = {}\n",
    "            stds[sample_name] = {}\n",
    "            # get values for each day\n",
    "            byDay = {}\n",
    "            for idx, x in enumerate(X):\n",
    "                # get data for this chunk\n",
    "                x = X[idx]\n",
    "                y = Y[idx]\n",
    "                day = days[idx]\n",
    "                #print(x, y, day)\n",
    "                country = countries[idx]\n",
    "                # log values to calc averages/stds from feature vectors\n",
    "                for j, x_j in enumerate(x):\n",
    "                    d = day[j]\n",
    "                    if country not in used:\n",
    "                        used[country] = {}\n",
    "                    if d not in used[country]:\n",
    "                        used[country][d] = True\n",
    "                        if d not in byDay:\n",
    "                            byDay[d] = []\n",
    "                        byDay[d].append(x_j)\n",
    "                if std_labels:\n",
    "                    # log values to calc averages/stds from labels\n",
    "                    for j, y_j in enumerate(y):\n",
    "                        d = day[len(x) + j]\n",
    "                        if country not in used:\n",
    "                            used[country] = {}\n",
    "                        if d not in used[country]:\n",
    "                            used[country][d] = True\n",
    "                            if d not in byDay:\n",
    "                                byDay[d] = []\n",
    "                            byDay[d].append(y_j)\n",
    "\n",
    "            # now transform data by calc means for each vector/label\n",
    "            for idx, x in enumerate(X):\n",
    "                # get data for this chunk\n",
    "                x = X[idx]\n",
    "                y = Y[idx]\n",
    "                day = days[idx]\n",
    "                try:\n",
    "                    # calc averages/stds of other days and transform feature vectors\n",
    "                    new_x = []\n",
    "                    for j, x_j in enumerate(x):\n",
    "                        d = day[j]\n",
    "                        sum_j = sum(byDay[d]) - x_j\n",
    "                        len_j = len(byDay[d]) - 1\n",
    "                        mean = sum_j / len_j\n",
    "                        std = ( (sum([((v - mean) ** 2) for v in byDay[d]]) - (x_j - mean) ** 2) / len_j) ** 0.5\n",
    "                        new_x.append((x_j - mean)/std)\n",
    "                        if isnan(new_x[j]) or isinf(new_x[j]):\n",
    "                            dummy = byDay[-2]\n",
    "                    # calc averages/stds of other days and transform labels\n",
    "                    new_y = []\n",
    "                    if std_labels:\n",
    "                        for j, y_j in enumerate(y):\n",
    "                            d = day[len(x) + j]\n",
    "                            sum_j = sum(byDay[d]) - y_j\n",
    "                            len_j = len(byDay[d]) - 1\n",
    "                            mean = sum_j / len_j\n",
    "                            std = ( (sum([((v - mean) ** 2) for v in byDay[d]]) - (y_j - mean) ** 2) / len_j) ** 0.5\n",
    "                            new_y.append((y_j - mean)/std)\n",
    "                            if isnan(new_y[j]) or isinf(new_y[j]):\n",
    "                                dummy = byDay[-2]\n",
    "                    else:\n",
    "                        new_y = y\n",
    "                except Exception:\n",
    "                    continue\n",
    "                else:\n",
    "                    new_Xs.append(new_x)\n",
    "                    new_Ys.append(new_y)\n",
    "\n",
    "                # calculate means and stds\n",
    "                for d in byDay:\n",
    "                    means[sample_name][d] = sum(byDay[d]) / len(byDay[d])\n",
    "                    stds[sample_name][d] = (sum([((v - means[sample_name][d]) ** 2) for v in byDay[d]]) / len(byDay[d])) ** 0.5\n",
    "\n",
    "        # transform data with passed in means and stds\n",
    "        else: \n",
    "            for idx, x in enumerate(X):  \n",
    "                # get data for this chunk\n",
    "                x = X[idx]\n",
    "                y = Y[idx]  \n",
    "                day = days[idx]\n",
    "                try:\n",
    "                    # transform feature vectors\n",
    "                    new_x = []\n",
    "                    for j, x_j in enumerate(x):\n",
    "                        d = day[j]\n",
    "                        new_x.append((x_j - means[sample_name][d])/stds[sample_name][d])\n",
    "                        if isnan(new_x[j]) or isinf(new_x[j]):\n",
    "                            dummy = byDay[-2]\n",
    "                    # transform labels\n",
    "                    new_y = []\n",
    "                    if std_labels:\n",
    "                        for j, y_j in enumerate(y):\n",
    "                            d = day[len(x) + j]\n",
    "                            new_y.append((y_j - means[sample_name][d])/stds[sample_name][d])\n",
    "                            if isnan(new_y[j]) or isinf(new_y[j]):\n",
    "                                dummy = byDay[-2]\n",
    "                    else:\n",
    "                        new_y = y\n",
    "                except Exception:\n",
    "                    continue\n",
    "                else:\n",
    "                    new_Xs.append(new_x)\n",
    "                    new_Ys.append(new_y)\n",
    "\n",
    "        #write pickle file\n",
    "        new_samples[sample_name] = {'data' : data, 'Xs' : new_Xs, 'Ys' : new_Ys, 'countries' : countries\n",
    "                                    , 'days' : days, 'params' : params, 'sample_name' : sample_name}\n",
    "        with open('sample_std_' + sample_name, 'wb') as outfile:\n",
    "            pickle.dump(new_samples[sample_name], outfile)\n",
    "\n",
    "    return new_samples, means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_IyJlS8MEQ23"
   },
   "outputs": [],
   "source": [
    "# runs cross-validation multiple times for all samples in output_samples\n",
    "# reads cross-validation results already ran from input_samples\n",
    "# @mlas is a list of string names of what mlas to use\n",
    "# @disp is a bool to either show output or not\n",
    "# function will return all output and input results and plot them\n",
    "def mla_regress_results(mlas, output_samples=[], input_sample_names=[], disp=True):\n",
    "    all_results = {}\n",
    "    \n",
    "    # run new and plot results, if samples dictionary is filled\n",
    "    warnings.filterwarnings('ignore') # ignore warnings that show when models do not converge\n",
    "    for sample_name in output_samples:\n",
    "        sample = output_samples[sample_name]\n",
    "        results = {}\n",
    "        results['sample_name'] = sample_name\n",
    "        if disp:\n",
    "            print('sample ' + sample_name + ' ...')\n",
    "        for mla in mlas:\n",
    "            if disp:\n",
    "                print('MLA ' + mla + ' ...')\n",
    "            results[mla] = crossValidation_regress(mla, sample['Xs'], sample['Ys'], 100, 5\n",
    "                                                     , (max(int((sample['params']['lags'])/2), 1)), disp )\n",
    "        all_results[sample_name] = results\n",
    "        if disp:\n",
    "            plotResults_regress(results, mlas) \n",
    "\n",
    "        #write pickle file with results\n",
    "        with open('MLA_regress_' + sample_name, 'wb') as outfile:\n",
    "            pickle.dump(results, outfile)\n",
    "\n",
    "    # read and plot results if inputs is not empty\n",
    "    for sample_name in input_sample_names:\n",
    "        with open('MLA_regress_' + sample_name, 'rb' ) as infile:\n",
    "            results = pickle.load(infile)\n",
    "            all_results[sample_name] = results\n",
    "            if disp:\n",
    "                plotResults_regress(results, mlas)\n",
    "                \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "G8Nft8zgEQ24"
   },
   "outputs": [],
   "source": [
    "# runs cross-validation multiple times for all samples in output_samples\n",
    "# reads cross-validation results already ran from inpu_samples\n",
    "# @mlas is a list of string names of what mlas to use\n",
    "# @disp is a bool to either show output or not\n",
    "# function will return all output and input results and plot them\n",
    "def mla_class_results(mlas, output_samples=[], input_sample_names=[], disp=True, multi=False):\n",
    "    all_results = {}\n",
    "    \n",
    "    # run new and plot results, if samples dictionary is filled\n",
    "    warnings.filterwarnings('ignore') # ignore warnings that show when models do not converge\n",
    "    for sample_name in output_samples:\n",
    "        sample = output_samples[sample_name]\n",
    "        results = {}\n",
    "        results['sample_name'] = sample_name\n",
    "        if disp:\n",
    "            print('sample ' + sample_name + ' ...')\n",
    "        for mla in mlas:\n",
    "            if disp:\n",
    "                print('MLA ' + mla + ' ...')\n",
    "            if multi:\n",
    "                results[mla] = crossValidation_class_multi(mla, sample['Xs'], sample['Ys'], 100, 5\n",
    "                                                     , (max(int((sample['params']['lags'])/2), 1)), disp )\n",
    "            else:\n",
    "                \n",
    "                results[mla] = crossValidation_class(mla, sample['Xs'], sample['Ys'], 100, 5\n",
    "                                                     , (max(int((sample['params']['lags'])/2), 1)), disp )\n",
    "        all_results[sample_name] = results\n",
    "        plotResults_class(results, mlas) \n",
    "\n",
    "        #write pickle file with results\n",
    "        with open('MLA_class_' + sample_name, 'wb') as outfile:\n",
    "            pickle.dump(results, outfile)\n",
    "\n",
    "    # read and plot results if inputs is not empty\n",
    "    for sample_name in input_sample_names:\n",
    "        with open('MLA_class_' + sample_name, 'rb' ) as infile:\n",
    "            results = pickle.load(infile)\n",
    "            all_results[sample_name] = results\n",
    "            if disp:\n",
    "                plotResults_class(results, mlas)\n",
    "                \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "nD_kohZuEQ24"
   },
   "outputs": [],
   "source": [
    "# reads pickle files of premade samples used MLA\n",
    "# @input_sample is list of file paths to pickle files\n",
    "# @returns list of samples\n",
    "def read_samples(sample_names):\n",
    "    samples = {}\n",
    "    for sample_name in sample_names:\n",
    "        with open('sample_' + sample_name, 'rb' ) as infile:\n",
    "            samples[sample_name] = pickle.load(infile)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "I3WVNyoKEQ24"
   },
   "outputs": [],
   "source": [
    "# runs cross validation of classification MLAs\n",
    "# @name is name of MLA (see if statements below)\n",
    "# @X is matrix of training features\n",
    "# @y are labels\n",
    "# @nIters is the number of times to re-run cross-validation\n",
    "# @nFolds is number of folds in cross-validation\n",
    "# @layers is a list of layer sizes to use for MLP\n",
    "# @disp will display results\n",
    "# returns list with average accuracy for each iteration\n",
    "def crossValidation_class_multi(name, X, y, nIters, nFolds, layers, disp=True):\n",
    "    \n",
    "    if disp:\n",
    "        dis = display(f'running MLA {name}',display_id=True)\n",
    "    \n",
    "    # create dummy Machine Learning Algorithm (MLA) object\n",
    "    mla = tree.DecisionTreeClassifier()\n",
    "    \n",
    "    # keep track of results\n",
    "    means = [None] * nIters\n",
    "    #stds = [None] * nIters\n",
    "    \n",
    "    # get results\n",
    "    for i in range(nIters):\n",
    "         # create MLA with new random seed\n",
    "        if 'DTr' in name:\n",
    "            mla = MultiOutputClassifier(DecisionTreeClassifier(random_state = i))\n",
    "        elif 'NBa' in name:\n",
    "            mla = MultiOutputClassifier(GaussianNB())\n",
    "        elif 'RFo' in name:\n",
    "            mla = MultiOutputClassifier(RandomForestClassifier(random_state = i, n_estimators=100))\n",
    "        elif 'ETr' in name:\n",
    "            mla = MultiOutputClassifier(ExtraTreesClassifier(random_state = i, n_estimators=100))\n",
    "        elif 'SVM' in name:\n",
    "            mla = MultiOutputClassifier(SVC(random_state = i, gamma='auto', probability=True))\n",
    "        elif 'KNN' in name:\n",
    "            mla = MultiOutputClassifier(KNeighborsClassifier(n_neighbors=2))\n",
    "        elif 'MLP' in name:\n",
    "            mla = MultiOutputClassifier(MLPClassifier(random_state = i, hidden_layer_sizes=layers, max_iter=10000))\n",
    "            \n",
    "        #print(X, y, 'EOL')\n",
    "        scores = cross_val_score(mla, X, np.array(y), cv=nFolds)\n",
    "        means[i] = scores.mean()\n",
    "        #stds[i] = std(scores)\n",
    "        if disp:\n",
    "            dis.update(f'iteration {i+1} of {nIters} done')\n",
    "    if disp:\n",
    "        dis.update(f'Average classification accuracy = {100. * sum(means) / len(means):.2f}%')\n",
    "        \n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cohzLCBwEQ24"
   },
   "outputs": [],
   "source": [
    "# runs cross validation of regression MLAs\n",
    "# @name is name of MLA (see if statements below)\n",
    "# @X is matrix of training features\n",
    "# @y are labels\n",
    "# @nIters is the number of times to re-run cross-validation\n",
    "# @nFolds is number of folds in cross-validation\n",
    "# @layers is a list of layer sizes to use for MLP\n",
    "# @disp will display results\n",
    "# returns list with average accuracy for each iteration\n",
    "def mla_regress_grid(sample, mlas = ['ENe', 'SVR', 'MLR', 'RFR']):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    X=sample['Xs']\n",
    "    Y=sample['Ys']\n",
    "    results = {}\n",
    "    for mla in mlas:\n",
    "        print ('ON MLA', mla)\n",
    "        results[mla] = {}\n",
    "        results[mla]['params'] = []\n",
    "        results[mla]['preds'] = []\n",
    "        if 'ENe' in mla:\n",
    "            for l1_ratio in [0, 0.2, 0.5, 0.8, 1.0]:\n",
    "                for fit_intercept in [True, False]:\n",
    "                    ml = ElasticNet(l1_ratio=l1_ratio, fit_intercept=fit_intercept)\n",
    "                    score = model_selection.cross_val_score(ml, X, Y, cv=5, scoring='r2')\n",
    "                    results[mla]['params'].append([l1_ratio, fit_intercept])\n",
    "                    print(results[mla]['params'][-1], score)\n",
    "                    results[mla]['preds'].append(score)\n",
    "        \n",
    "        elif 'SVR' in mla:\n",
    "            for kernel in ['rbf', 'linear', 'poly', 'sigmoid']:\n",
    "                for degree in [2, 4, 8]:\n",
    "                    for C in [0.1, 1, 10, 100, 1000]:\n",
    "                        for shrinking in [True, False]:\n",
    "                            ml = SVR(max_iter=200, gamma='scale', kernel=kernel, degree=degree, C=C, shrinking=shrinking)\n",
    "                            score = model_selection.cross_val_score(ml, X, Y, cv=5, scoring='r2')\n",
    "                            results[mla]['params'].append([kernel, degree, C, shrinking])\n",
    "                            print(results[mla]['params'][-1], score)\n",
    "                            results[mla]['preds'].append(score)\n",
    "                                        \n",
    "        \n",
    "        elif 'RFR' in mla:\n",
    "            for n_estimators in [10, 100, 1000]:\n",
    "                for max_depth in [1, 2, 4, 8, 16]:\n",
    "                    for bootstrap in [True, False]:\n",
    "                        for min_impurity_split in [1e-8, 1e-6, 1e-4, 1e-2]:\n",
    "                            ml = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, bootstrap=bootstrap\n",
    "                                                       , min_impurity_split=min_impurity_split)\n",
    "                            score = model_selection.cross_val_score(ml, X, Y, cv=5, scoring='r2')\n",
    "                            results[mla]['params'].append([n_estimators, max_depth, bootstrap, min_impurity_split])\n",
    "                            print(results[mla]['params'][-1], score)\n",
    "                            results[mla]['preds'].append(score)\n",
    "        \n",
    "        elif 'MLR' in mla:\n",
    "            for hidden_layer_sizes in [[16], [32], [64], [32, 16], [64, 32, 16], [64, 32]]:\n",
    "                for activation in ['logistic', 'tanh', 'relu']:\n",
    "                    for solver in ['lbfgs', 'sgd', 'adam']:\n",
    "                        for alpha in [1e-6, 1e-4, 1e-2]:\n",
    "                            for learning_rate in ['constant', 'invscaling', 'adaptive']:\n",
    "                                for max_iter in [200, 500, 1000]:\n",
    "                                    for early_stopping in [True, False]:\n",
    "                                        ml = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, activation=activation\n",
    "                                                          , solver=solver, alpha=alpha, learning_rate=learning_rate\n",
    "                                                          , max_iter=max_iter, early_stopping=early_stopping)\n",
    "                                        print(np.isnan(X).any(), np.isnan(Y).any())\n",
    "                                        score = model_selection.cross_val_score(ml, X, Y, cv=5, scoring='r2')\n",
    "                                        results[mla]['params'].append([hidden_layer_sizes, activation, solver, alpha\n",
    "                                                                       , learning_rate, max_iter, early_stopping])\n",
    "                                        print(results[mla]['params'][-1], score)\n",
    "                                        results[mla]['preds'].append(score)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "FPPbROTGEQ25"
   },
   "outputs": [],
   "source": [
    "# runs cross validation of regression MLAs\n",
    "# @name is name of MLA (see if statements below)\n",
    "# @X is matrix of training features\n",
    "# @y are labels\n",
    "# @nIters is the number of times to re-run cross-validation\n",
    "# @nFolds is number of folds in cross-validation\n",
    "# @layers is a list of layer sizes to use for MLP\n",
    "# @disp will display results\n",
    "# returns list with average accuracy for each iteration\n",
    "def crossValidation_regress(name, X, y, nIters, nFolds, layers, disp=True):\n",
    "    if disp:\n",
    "        dis = display(f'running MLA {name}',display_id=True)\n",
    "    y = [x[0] for x in y]\n",
    "    # create dummy Machine Learning Algorithm (MLA) object\n",
    "    kfold = model_selection.KFold(n_splits=nFolds)\n",
    "    scoring = 'r2'\n",
    "    mla = LinearRegression()\n",
    "    \n",
    "    # keep track of results\n",
    "    means = [None] * nIters\n",
    "    #stds = [None] * nIters\n",
    "    \n",
    "    # get results\n",
    "    for i in range(nIters):\n",
    "         # create MLA with new random seed\n",
    "        if 'LRe' in name:\n",
    "            mla = LinearRegression()\n",
    "        elif 'Rid' in name:\n",
    "            mla = Ridge(alpha=0.1, random_state = i)\n",
    "        elif 'Las' in name:\n",
    "            mla = Lasso(alpha=0.1, random_state = i)\n",
    "        elif 'ENe' in name:\n",
    "            mla = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state = i)\n",
    "        elif 'SVR' in name:\n",
    "            mla = SVR(kernel='linear')\n",
    "        elif 'MLR' in name:\n",
    "            mla = MLPRegressor(random_state = i, hidden_layer_sizes=layers, max_iter=1000)\n",
    "        elif 'RFR' in name:\n",
    "            mla = RandomForestRegressor(random_state=i, n_estimators=100)\n",
    "            \n",
    "        #print(X, y, 'EOL')\n",
    "        results = model_selection.cross_val_score(mla, X, y, cv=kfold, scoring=scoring)\n",
    "        means[i] = results.mean()\n",
    "        #stds[i] = results.std()\n",
    "        if disp:\n",
    "            dis.update(f'iteration {i+1} of {nIters} done')\n",
    "    if disp:\n",
    "        dis.update(f'Average R2 score = {sum(means) / len(means):.2f}')\n",
    "        \n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pmpyNcJzEQ25"
   },
   "outputs": [],
   "source": [
    "# runs cross validation of classification MLAs\n",
    "# @name is name of MLA (see if statements below)\n",
    "# @X is matrix of training features\n",
    "# @y are labels\n",
    "# @nIters is the number of times to re-run cross-validation\n",
    "# @nFolds is number of folds in cross-validation\n",
    "# @layers is a list of layer sizes to use for MLP\n",
    "# @disp will display results\n",
    "# returns list with average accuracy for each iteration\n",
    "def crossValidation_class(name, X, y, nIters, nFolds, layers, disp=True):\n",
    "    \n",
    "    if disp:\n",
    "        dis = display(f'running MLA {name}',display_id=True)\n",
    "    \n",
    "    # create dummy Machine Learning Algorithm (MLA) object\n",
    "    mla = tree.DecisionTreeClassifier()\n",
    "    \n",
    "    # keep track of results\n",
    "    means = [None] * nIters\n",
    "    #stds = [None] * nIters\n",
    "    \n",
    "    # get results\n",
    "    for i in range(nIters):\n",
    "         # create MLA with new random seed\n",
    "        if 'DTr' in name:\n",
    "            mla = DecisionTreeClassifier(random_state = i)\n",
    "        elif 'LRe' in name:\n",
    "            mla = LogisticRegression()\n",
    "        elif 'NBa' in name:\n",
    "            mla = GaussianNB()\n",
    "        elif 'RFo' in name:\n",
    "            mla = RandomForestClassifier(random_state = i, n_estimators=100)\n",
    "        elif 'ETr' in name:\n",
    "            mla = ExtraTreesClassifier(random_state = i, n_estimators=100)\n",
    "        elif 'SVM' in name:\n",
    "            mla = SVC(random_state = i, gamma='auto', probability=True)\n",
    "        elif 'KNN' in name:\n",
    "            mla = KNeighborsClassifier(n_neighbors=2)\n",
    "        elif 'MLP' in name:\n",
    "            mla = MLPClassifier(random_state = i, hidden_layer_sizes=[], max_iter=10000)\n",
    "            \n",
    "        #print(X, y, 'EOL')\n",
    "        scores = cross_val_score(mla, X, y, cv=nFolds)\n",
    "        means[i] = scores.mean()\n",
    "        #stds[i] = std(scores)\n",
    "        if disp:\n",
    "            dis.update(f'iteration {i+1} of {nIters} done')\n",
    "    if disp:\n",
    "        dis.update(f'Average classification accuracy = {100. * sum(means) / len(means):.2f}%')\n",
    "        \n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "d_zRcg7xEQ25"
   },
   "outputs": [],
   "source": [
    "# plots the results returned from crossValidation_regress()\n",
    "# @results are results returned from crossValidation_regress()\n",
    "# @mlas are list of mlas used in results to output\n",
    "def plotResults_regress(results, mlas):\n",
    "    x_labels = []\n",
    "    for mla in mlas:\n",
    "        x_labels.append(mla)\n",
    "        x_labels.append(' ')\n",
    "        x_labels.append(' ')\n",
    "        x_labels.append(' ')\n",
    "    x_nums = [x for x in range(5*len(mlas))]\n",
    "    fig = plt.figure(figsize=(7.5,5))\n",
    "    plt.xticks(x_nums, x_labels, rotation='vertical', fontsize=20)\n",
    "    plt.yticks([x/100 for x in range(0, 101, 10)], fontsize=20)\n",
    "    plt.tick_params(axis='x', length=0)\n",
    "    plt.ylabel('R2 Score', fontsize=20)\n",
    "    plt.title('10-Fold Cross-Validation Estimating Number of Cases - Sample ' + results['sample_name'], fontsize=20)\n",
    "    plt.grid(axis='y', linewidth=0.4)\n",
    "    plt.ylim([0.00, 1.01])\n",
    "    offset = 0\n",
    "    for mla in mlas:\n",
    "        x = [offset for _ in range(len(results[mla]))]\n",
    "        y = np.array(results[mla])\n",
    "        if max(y) <= 0.00:\n",
    "            plt.scatter(offset+0, .01, color='red', marker='v')\n",
    "        else:\n",
    "            plt.scatter(x, y, color='green', marker='_', s=1000)\n",
    "        plt.axvline(x=offset, color='grey', linewidth=0.3, alpha=0.3)\n",
    "        offset += 4\n",
    "    fig.text(0, -.20, 'Illustrates distribution of R2 scores for each MLA.\\nResults from 100 random runs.\\nA red triangle indicates results below 0'\n",
    "             , fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "UUFzTO0kEQ25"
   },
   "outputs": [],
   "source": [
    "# plots the results returned from crossValidation_class()\n",
    "# @results are results returned from crossValidation_class()\n",
    "# @mlas are list of mlas used in results to output\n",
    "def plotResults_class(results, mlas):\n",
    "    x_labels = []\n",
    "    for mla in mlas:\n",
    "        x_labels.append(mla)\n",
    "        x_labels.append(' ')\n",
    "        x_labels.append(' ')\n",
    "        x_labels.append(' ')\n",
    "    x_nums = [x for x in range(5*len(mlas))]\n",
    "    fig = plt.figure(figsize=(7.5,5))\n",
    "    plt.xticks(x_nums, x_labels, rotation='vertical', fontsize=20)\n",
    "    plt.yticks([x for x in range(70, 101, 5)], fontsize=20)\n",
    "    plt.tick_params(axis='x', length=0)\n",
    "    plt.ylabel('% Classification Accuracy', fontsize=20)\n",
    "    plt.title('10-Fold Cross-Validation Incrase or Decrease in Cases - Sample ' + results['sample_name'], fontsize=20)\n",
    "    plt.grid(axis='y', linewidth=0.4)\n",
    "    plt.ylim([70, 101])\n",
    "    offset = 0\n",
    "    for mla in mlas:\n",
    "        x = [offset for _ in range(len(results[mla]))]\n",
    "        y = 100. * np.array(results[mla])\n",
    "        if int(max(y)) <= 70:\n",
    "            plt.scatter(offset+0, 71, color='red', marker='v')\n",
    "        else:\n",
    "            plt.scatter(x, y, color='green', marker='_', s=1000)\n",
    "        plt.axvline(x=offset, color='grey', linewidth=0.3, alpha=0.3)\n",
    "        offset += 4\n",
    "    fig.text(0, -.20, 'Illustrates distribution of accuracies for each MLA.\\nResults from 100 random runs.\\nA red triangle indicates results below 70% accuracy'\n",
    "             , fontsize=20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "utils.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
